---
title: "Calibration notes"
author: "K. Davidson"
date: "Last update: `r Sys.Date()`"
output: 
  html_document: 
    df_print: kable
---

--------

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries 
library(tidyverse)
library(readxl)    # to read in .xlsx files
library(egg)       # for ggarrange()
library(ggpubr)    # for stat_cor() in ggplot theme
library(scales)    # for pretty_breaks()
library(finalfit)  # for ff_glimpse()/missing values summaries
library(naniar)    # for miss_var_summary()
library(car)       # for vif()
#library(glmmTMB)  # for glmmTMB()
#library(mgcv)     # do not run if gam is loaded. for gam() with regression splines
#library(gam)      # do not run if mgcv is loaded. for gam() with loess
#library(lme4) 
library(MuMIn)     # for dredge(), model.avg(), predict(), etc.


# read in data
setwd("~/ANALYSIS/data")
cal.raw <- read_excel("calibration_2020_KDupdate.xlsx", sheet="Calibration_v3")        #***copy-pasted to wd() Jan18, may require updating***
options(scipen = 999)


# cleaning & calculating 
cal <- cal.raw %>% 
  mutate(across(c("bankfull":"water_vis"), ~ifelse(.x=="NA", NA, .x))) %>%
  mutate_at(c("water_level", "water_discharge", "OE", "ground_cuml_carc_above", "peak_live_below", "ground_cuml_carc_above", "ground_cuml_carc_below", "aerial_unsexed_above", "aerial_unsexed_below", "ground_carcs_total_above", "hpe_adj"), as.numeric) %>%
  mutate_at(c("system_stability", "size", "water_clarity", "substrate_shade", "canopy_cover", "large_woody_debris", "lpe_method", "bankfull", "brightness", "cloud_cover", "fish_vis", "water_vis"), as.factor) %>%
  mutate(lpe = ifelse(lpe_method=="Aerial" & !is.na(aerial_unsexed_above), peak_live_above+aerial_unsexed_above, 
    ifelse(lpe_method=="Aerial" & is.na(aerial_unsexed_above), peak_live_above,
      ifelse(lpe_method=="Ground" & is.na(ground_cuml_carc_above), peak_live_above,
        ifelse(lpe_method=="Ground" & !is.na(ground_cuml_carc_above) & hpe_method!="MR", peak_live_above+ground_cuml_carc_above, 
          ifelse(lpe_method=="Ground" & !is.na(ground_cuml_carc_above) & hpe_method=="MR", peak_live_above, NA)))))) %>%
  mutate(index = hpe/lpe) %>%
  mutate(sid = paste(gazetted_stream_name, year, sep="-")) %>%
  mutate(usid = paste(gazetted_stream_name, year, lpe_method, sep="-")) %>%
  mutate(n_row = 1:nrow(.)) %>%
  mutate(size_recode = case_when(size=="V. Small"~1, size=="Small"~2, size=="Medium"~3, size=="Large"~4, size=="X-Large"~5)) %>%
  mutate(water_clarity_recode = case_when(water_clarity=="Clear"~1, water_clarity=="Pt. Turbid / Tannic"~2, water_clarity=="Turbid"~3)) %>%
  mutate(substrate_shade_recode = case_when(substrate_shade=="Light"~1, substrate_shade=="Medium"~2, substrate_shade=="Dark"~3)) %>%
  mutate(canopy_cover_recode = case_when(canopy_cover=="Low"~1, canopy_cover=="Medium"~2, canopy_cover=="High"~3)) %>%
  mutate(lwd_recode = case_when(large_woody_debris=="Low"~1, large_woody_debris=="Medium"~2, large_woody_debris=="High"~3)) %>%
  mutate(bankfull_recode = case_when(bankfull=="0-25"~1, bankfull=="25-50"~2, bankfull=="50-75"~3, bankfull=="75-100"~4)) %>%
  mutate(brightness_recode = case_when(brightness=="Dark"~1, brightness=="Medium"~2, brightness=="Bright"~3, brightness=="Full"~4)) %>%
  mutate(cc_recode = case_when(cloud_cover=="0"~1, cloud_cover=="25"~2, cloud_cover=="50"~3, cloud_cover=="75"~4, cloud_cover=="100"~5)) %>%
  mutate(brightness_recode = case_when(brightness=="Dark"~1, brightness=="Medium"~2, brightness=="Bright"~3, brightness=="Full"~4)) %>%
  mutate(fish_vis_recode = case_when(fish_vis=="Low"~1, fish_vis=="Medium"~2, fish_vis=="High"~3)) %>%
  mutate(water_vis_recode = case_when(water_vis=="0.25-0.5"~1, water_vis=="0.5-1.0"~2, water_vis=="1.0-3.0"~3, water_vis=="3.0-bottom"~4)) %>%
  mutate(stability_recode = case_when(system_stability=="Stable"~0, system_stability=="Volatile"~1)) %>%
  mutate(lpe_method_recode = case_when(lpe_method=="Aerial"~1, lpe_method=="Ground"~0)) %>%
  mutate_at(c("water_clarity_recode", "substrate_shade_recode", "lwd_recode", "fish_vis_recode"), as.integer) %>%
  mutate(lpe_sc = scale(lpe, center=F, scale=T)) %>%
  dplyr::select(sid, usid, n_row, watershed_group:lpe_sc)
cal$size <- factor(cal$size, levels=c(NA, "V. Small", "Small", "Medium", "Large", "X-Large"), ordered=T)

# export for summary version - Aug 2021
# write.csv(cal, "calibration_clean_BI02sub.csv", row.names=F)
```

<br>

<br>

<br>

----------------

## **Preliminary results** ##

----------------

### **Multicollinearity and variable relationships (only static variables)** ### 

---------

*Missing values assessment not needed (see Fig A0.); Data exploration completed in Appendix 2.*

#### **Multicollinearity** ####  

```{r ehco=F, include=F}
# FUNCTIONS FOR THIS SECTION: 

#-------- Pairs plot functions
# Function for scaled correlation coefficient text in upper panels
panel.cor.fx <- function(x, y, digits=2, prefix="", cex.cor, ...)
{
  usr = par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r = cor(x, y,use="na.or.complete")
  txt = format(c(r, 0.123456789), digits=digits)[1]
  txt = paste(prefix, txt, sep="")
  if(missing(cex.cor)) cex.cor = 0.8/strwidth(txt)
  text(0.5, 0.5, txt, cex=ifelse(r<0.3 & r>-0.3, 0.7, r*2))#cex.cor * r)    (r^2)*2.5
}

# Function for data histograms on diagonal
panel.hist.fx <- function(x, ...)
{
    usr <- par("usr"); on.exit(par(usr))
    par(usr = c(usr[1:2], 0, 1.5) )
    h <- hist(x, plot = FALSE)
    breaks <- h$breaks; nB <- length(breaks)
    y <- h$counts; y <- y/max(y)
    rect(breaks[-nB], 0, breaks[-1], y, col = "blue", ...)
}
```

```{r echo=F, include=F}
# Pairs COR
z.s <- cbind(cal$index, cal$system_stability, cal$size_recode, cal$water_clarity_recode, cal$substrate_shade_recode, cal$canopy_cover_recode, cal$lwd_recode, cal$lpe, cal$lpe_method)   
colnames(z.s) <- c("index", "stability", "size", "water_clarity", "substrate_shade", "canopy_cover", "lwd", "lpe", "lpe_method")
```

Examination of static variables only indicates high collinearity between several variables (*r*>0.6; Figure 3). However, in the initial global model, VIF scores are exceptionally high for stream size (Table 1a). Step-wise variable removal based on highest VIF values indicates that the best combination of explanatory variables to reduce multicollinearity moving forward is: stability, water clarity, substrate shade, LWD, LPE, and LPE method; i.e., removing stream size. Note that canopy cover was not considered moving forward as it can vary annually with fires, pests, etc.

```{r echo=F, warning=F, message=F}
pairs(z.s, lower.panel=panel.smooth, upper.panel=panel.cor.fx, diag.panel=panel.hist.fx, cex=1, pch=16)
```

<font size="2"> *Fig. 3. Pairs plot for STATIC variables indicating level of collinearity between each variable pairing. Histograms of data observations given on the diagonal. Smoothed lines (red) generated using default loess smoother. No data transformations. Ordinal categorical variables re-coded as integers for purposes of plotting.* </font>

<br>

```{r include=F, echo=F}
#--------- VIF tests - stepwise removal of highly collinear predictors (cutoff VIF>3)
### just static vars

# global
vif.stat.1 <- lm(index ~ system_stability + size_recode + water_clarity_recode + substrate_shade_recode + lwd_recode + lpe + lpe_method, data=cal)

#no size
vif.stat.2 <- lm(index ~ system_stability + water_clarity_recode + substrate_shade_recode + lwd_recode + lpe + lpe_method, data=cal)       
```

<font size="2"> *Table 1. Variance Inflation Factor (VIF) scores for global models with STATIC variables. Note Table 3a shows the VIF scores for the global model; Table 3b is not shown but removes the highest VIF score to result in the remaining variables exhibiting VIF < 3.* </font>

```{r echo=F, warning=F, message=F}
knitr::kable(cbind("Table"=c("3a", "(global)", rep("",5)), dplyr::as_data_frame(vif(vif.stat.1), rownames = "Variable")))
#knitr::kable(cbind("Table"=c("3b", "(global)", "(-size)", rep("",3)), dplyr::as_data_frame(vif(vif.stat.2), rownames = "Variable")))
```

<br>

#### **Relationships** ####  

```{r echo=F, include=F}
# FUNCTIONS FOR THIS SECTION: 

#-------- Custom relationship plot function
relation.fx <- function(x_var, ann_x, ann_y, ann_lab, x_lab){
  ggplot(cal%>%filter(!is.na(.data[[x_var]]), !grepl("CANT", .data[[x_var]])), aes(x=.data[[x_var]], y=index)) +
    geom_point(shape=21, size=3, fill="gray70", alpha=0.6) +
    geom_smooth(method = "lm", se=F, colour="green", size=1, alpha=0.5) +
    geom_smooth(method = "gam", formula = y ~ s(x, bs = "cs"), se=F, colour="red", size=1, alpha=0.5) +
    geom_smooth(method = "loess", se=F, colour="blue", size=1, alpha=0.5) +
    annotate("text", x=ann_x, y=ann_y, label=ann_lab, fontface=2) +
    labs(x=x_lab, y="Index") +
    theme_bw() + 
    theme(legend.position=c(0.1,0.85),
      legend.background = element_rect(colour="black"),
      legend.spacing.y = unit(0.1, "mm"),
      legend.spacing.x = unit(0.1, "mm"),
      legend.margin=margin(t=0.1, r=0.1, b=0.1, l=0.1, unit="cm"),
      legend.key.size = unit(0.5, "cm"),
      axis.text=element_text(colour="black"))
  }
```

There is evidence of relationships between the index and most variables (Fig 4). Smoothed lines are shown just for comparison to previous graphs and for future considerations with GAM use, but are not considered in this GLM exercise. 

```{r echo=F, warning=F, message=F}
ggarrange(relation.fx("system_stability", "Stable", 17, "", "Stability"),
          relation.fx("water_clarity_recode", 1, 17, "", "Water clarity"), 
          relation.fx("substrate_shade_recode", 1, 17, "", "Substrate shade"),
          relation.fx("lwd_recode", 1, 17, "", "LWD"),
          relation.fx("lpe", 1, 17, "", "LPE"),
          relation.fx("lpe_method", 1, 17, "", "LPE method"),
          common.legend = T)
```

<font size="2"> *Fig 4. Relationship plots for static variables following step-wise VIF variable removal. Lines are linear (green), cubic spline (red) and loess (blue) smoothing.* </font>

<br>

<br>

<br>

### **Model development** ###

----------------

The following is simply an exercise in model development; it does not address how to handle missing values. Previous work with GAMs appears to have issues with the level of predictors and effective degrees of freedom, models do not converge. Attempt with GLMM and static variables only.

Based on the VIF process, the predictor variables are:    

* Stability    
* Water clarity   
* Substrate shade   
* LWD   
* LPE   
* LPE method    

To test this process, the model was fit to a random selection of 80% of the data and validated on a random 20%.

```{r eval=F}
Global model formula: 
m.global <- glm(index ~ system_stability + large_woody_debris + water_clarity + substrate_shade + lpe_sc + lpe_method, data=cal.80,  family=Gamma(link="inverse"), na.action = "na.fail")
```

```{r echo=F, include=F}
# Randomly select 80% of data. MUST RUN set.seed(1) first for reproducibility! 
# note addition of term from below - rescaling 'lpe'
set.seed(1)
cal.80 <- cal %>% 
  slice_sample(prop=0.8) %>%
  mutate(hpe = ifelse(is.na(hpe), hpe_escdb, hpe)) %>%
  mutate(index = ifelse(is.na(index), hpe/lpe, index)) %>%
  print()
cal.80.names <- cal.80$usid

# Filter remaining 20% of data for model validation
cal.20 <- cal %>%
  filter(!usid%in%c(cal.80.names)) %>%
  print()

#--------- FIT MODELS
# model suggests re-scaling 'lpe' - see above code chunk 

m.global <- glm(index ~ system_stability + large_woody_debris + water_clarity + substrate_shade + lpe_sc + lpe_method, data=cal.80,  family=Gamma(link="inverse"), na.action = "na.fail")
summary(m.global)

# Run all combinations of models
m.dredge <- dredge(m.global, beta="none", rank="AICc")
summary(m.dredge)


#--------- EVALUATE MODELS
# 1.1. TOP MODEL SET: dAIC<4  
model.sel.dAIC4 <- get.models(m.dredge, subset=delta<4)                    # note essentially identical outcome with weight<=0.95
models4 <- model.sel(model.sel.dAIC4)
model.table.dAIC4 <- subset(m.dredge, delta<4)                             # just model table for different viewing options
# 1.2. MODEL-AVERAGE TOP MODEL SET: dAIC<4
model.avg.dAIC4 <- model.avg(model.sel.dAIC4)

# 2. TOP MODEL: dAIC=0
model.sel.top <- get.models(m.dredge, subset=1)
m.top <- glm(index ~ large_woody_debris + lpe_sc + system_stability + water_clarity, data=cal.80, family=Gamma(link="inverse"), na.action="na.fail")

# Model tables for visualizing
model.table.dAIC4 <- subset(m.dredge, delta<4)
model.table.top <- subset(m.dredge, subset=1)


#--------- PREDICT/VALIDATE MODEL
# Based on model-averaged parameters: 
predictions.avgmod4 <- predict(model.avg.dAIC4, type="link", backtransform=T, full=T, cal.20, se.fit=T)
cal.20$index_avgmod4 <- predictions.avgmod4[[1]]
cal.20$index_avgmod4.se <- predictions.avgmod4[[2]]

predictions.topmod <- predict(m.top, type="link", backtransform=T, full=T, cal.20, se.fit=T)
cal.20$index_topmod <- predictions.topmod[[1]]
cal.20$index_topmod.se <- predictions.topmod[[2]]
```

Models were ranked based on AIC corrected for small sample size (AICc), and top models were identified as those with dAICc < 4 (Table 2). The top model (dAICc=0) contained terms for LWD, LPE, system_stability, and water clarity (LPE method and substrate_shade were excluded). The top model had approximately 1.5-times the weight of evidence compared to the second model (w=0.33 and w=0.21, respectively). The top model accounted for 33% of the weight of evidence in the top set of models (Table 2). 

<br>

<font size="2"> *Table 2. Set of top models with dAICc ("delta") < 4.* </font>

```{r echo=F, warning=F, message=F}
# Top models table
knitr::kable(model.table.dAIC4)
#knitr::kable(cbind("Model"=row.names(models4), dplyr::as_data_frame(models4)%>%select(-family)))
```

<br>

From the top model set we were able to calculate Relative Variable Importance (RVI) values as we fit a balanced set of candidate models. Large woody debris, LPE and system_stability were all the most important (RVI=1), which was 1.3-times as important as water clarity, and approx. 2.5 to 3 times as important as substrate shade and LPE method, respectively (Table 3). It is surprising to have 3 variables tie for complete importance (RVI=1). 

<br>

<font size="2"> *Table 3. Relative variable importance of variables included in the model-averaged set of top models (dAICc < 4).*</font>

```{r echo=F, warning=F, message=F}
# RVI table
knitr::kable(cbind("Variable name"=rownames(as.data.frame(cbind(model.avg.dAIC4$sw))),
                   as.data.frame(cbind(model.avg.dAIC4$sw),row.names=F)%>%rename(`Relative importance`=V1)))
```

Due to the relatively high support for the top model, and a reasonable selection of models comprising the set of top models (dAIC < 4), we predicted expansion factors from both the top model and from model-averaging the set of top models. Model averaging is typically the preferred method of choosing model parameters as it retains more complexity and parameters to explain ecological phenomena. However, as we are interested in the best predictive model, rather than the most robust explanatory power, we assessed both methods for deriving model-predicted expansion factors. 

```{r echo=F, include=F}
####### Plot background work, not shown ###
# PLOT PREDICTIONS: INDICES AND POPULATION SIZES
# "True" index (hpe/lpe) vs. modelled index - note top model performs poorly compared to model-averaged and is not shown
index<-ggplot() +
  geom_hline(yintercept = 1.8, colour="red") +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=index), stat="identity", colour="black", fill="white") +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=index_avgmod4), stat="identity", fill="blue", alpha=0.8, width=0.7) +
  theme_bw() 

ggplot(cal.20) +
  geom_point(aes(x=factor(n_row), y=index_topmod)) +
  geom_errorbar(aes(x=factor(n_row), ymin=index_topmod-index_topmod.se, ymax=index_topmod+index_topmod.se)) +
  geom_point(aes(x=factor(n_row), y=index_avgmod4), fill="red") +
  geom_errorbar(aes(x=factor(n_row), ymin=index_avgmod4-index_avgmod4.se, ymax=index_avgmod4+index_avgmod4.se), colour="red")

# Implications of "true" population size (HPE) vs. resulting 1.8-expanded HPE and model-expanded HPE
cal.20 <- cal.20 %>% 
  mutate(pop_est_18 = lpe*1.8,
         pop_est_avgmod4 = lpe*index_avgmod4,
         pop_est_topmod = lpe*index_topmod,
         pop_se_avgmod4 = lpe*index_avgmod4.se,
         pop_se_topmod = lpe*index_topmod.se)

pop_ests<-ggplot() +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=hpe), stat="identity", colour="gray70", fill="gray70", width=0.8) +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=pop_est_18), stat="identity", colour="black", fill="white", alpha=0.5, width=0.6) +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=pop_est_avgmod4), stat="identity", fill="blue", alpha=0.5, width=0.6) +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=pop_est_topmod), stat="identity", fill="green", alpha=0.5, width=0.6) +
  labs(y="Population estimate", x="Survey")


# PLOT PREDICTIONS: DIFFERENCES BETWEEN ESTIMATES
# Only doing top model set as the top model solo didn't perform well. 
DBEs <- cal.20 %>%
  dplyr::select(usid, n_row, hpe, pop_est_18, pop_est_avgmod4) %>%
  pivot_longer(cols=c(pop_est_18, pop_est_avgmod4), names_to = "expand_type", values_to = "pop_est") %>%
  mutate(difference=hpe-pop_est,
         pDBE=(difference/hpe)*100) %>%
  group_by(usid) %>%
  mutate(winner = min(abs(difference))) %>%
  mutate(winner = ifelse(winner==difference, "WIN", ifelse(winner*(-1)==difference, "WIN", NA))) %>%
  print()

dDBE<-ggplot(DBEs, aes(x=factor(n_row), y=difference, group=expand_type, fill=expand_type, colour=winner)) +
  geom_hline(yintercept = 0, colour="black") +
  geom_bar(stat="identity", position="dodge", width=0.5) +
  scale_fill_manual(labels = c("lpe*1.8", "lpe*model-averaged predicted index"), values=c("gray70", "turquoise")) +
  scale_colour_manual(labels = c("WIN", NA), values=c("black", NULL), guide=F) +
  labs(y="# over-est        # under-est", fill="expansion method", x="Survey") +
  theme_bw() +
  theme(legend.position = c(0.35,0.15),
        legend.background = element_rect(colour="black"),
        legend.key.height = unit(2, "mm"),
        legend.key.width = unit(3, "mm"),
        legend.text = element_text(size=8),
        legend.title = element_text(size=9, face="bold"))

pDBE<-ggplot(DBEs, aes(x=factor(n_row), y=pDBE, group=expand_type, fill=expand_type, colour=winner)) +
  geom_hline(yintercept = 0, colour="black") +
  geom_bar(position="dodge", stat="identity", width=0.5) +
  scale_fill_manual(labels = c("lpe*1.8", "lpe*model-averaged predicted index"), values=c("gray70", "turquoise")) +
  scale_colour_manual(labels = c("WIN", NA), values=c("black", NULL), guide=F) +
  labs(y="% over-est      % under-est", fill="expansion method", x="Survey",
       caption="Fig 6. Discrepancy in a) number of fish and b) proportion of population between the 1.8-expanded population \nestimate and the model-estimated population estimate compared to the known HPE. The winning result is outlined \nin black.") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0, face="italic"),
        legend.position = c("none"))

##### 3. SUMMARY STATS: 
# cases where model was best based on # fish (not proportion - but gives the same result in this case)
winners <- DBEs %>%
  filter(winner=='WIN') %>%
  group_by(expand_type) %>%
  summarize(n=n()) %>%
  print()
#######
#######
```

<br>

<span style="text-decoration:underline"> **Model validation** </span>

Based on the two methods of model-derived expansion factors (top model and model-averaged set), it is apparent that model-averaging the top set of models (dAIC < 4) provided better expanded population estimates compared to the top model, which consistently underestimated the population size (Figure 5). When assessing the differences in population estimate (i.e., number of fish) between the known population estimate and the two methods of expansion factors, the model-averaged predicted expansion factor performed better than the 1.8 expansion factor in `r winners[winners$expand_type=="pop_est_avgmod4",]$n` out of `r sum(winners$n)` cases (`r round((winners[winners$expand_type=="pop_est_avgmod4",]$n/sum(winners$n))*100, 0)`%), i.e., the predicted population estimate was closer to the real population estimate (Figure 5). The model over-estimated the population size for `r pull(DBEs%>%filter(expand_type=="pop_est_avgmod4"&difference<0)%>%ungroup()%>%summarize(n=n()))` surveys and underestimated the population size for `r pull(DBEs%>%filter(expand_type=="pop_est_avgmod4"&difference>0)%>%ungroup()%>%summarize(n=n()))` surveys. Similarly, the 1.8-expansion factor over-estimated for `r pull(DBEs%>%filter(expand_type=="pop_est_18"&difference<0)%>%ungroup()%>%summarize(n=n()))` surveys and underestimated for `r pull(DBEs%>%filter(expand_type=="pop_est_18"&difference>0)%>%ungroup()%>%summarize(n=n()))` surveys. 

```{r echo=F, warning=F, message=F}
ggplot() +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=hpe), stat="identity", fill="gray60",  alpha=0.7) +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=pop_est_18), stat="identity", fill="white", alpha=0.4) +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=pop_est_avgmod4), stat="identity", fill="turquoise", colour="blue", alpha=0.7, width=0.5) +
  geom_bar(data=cal.20, aes(x=factor(n_row), y=pop_est_topmod), stat="identity", fill="red", colour="black", alpha=0.7, width=0.5) +
  
  geom_errorbar(data=cal.20, aes(x=factor(n_row), ymin=pop_est_avgmod4-pop_se_avgmod4, ymax=pop_est_avgmod4+pop_se_avgmod4), 
                stat="identity", colour="blue", width=0.3) +
  geom_errorbar(data=cal.20, aes(x=factor(n_row), ymin=pop_est_topmod-pop_se_topmod, ymax=pop_est_topmod+pop_se_topmod), 
                stat="identity", colour="black", width=0.3) +
  
  #geom_point(data=DBEs%>%group_by(usid)%>%mutate(winner = ifelse(min(difference)==difference, "WIN", ""))%>%filter(winner=='WIN'), 
  #           aes(x=factor(n_row), y=))
  labs(y="Population estimate", x="Survey number", caption="Fig 5. High-precision population estimate (dark gray) compared to the low-precision estimate expanded by 1.8 \nexpansion factor (light gray), the top model predicted expansion factor (red), and the model-averaged predicted \nexpansion factor (turquoise) for 20% of the dataset. Prediction standard error bars are given.") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0, face="italic"),
        axis.text = element_text(colour="black"),
        axis.title = element_text(face="bold"))
```

<br>

We compared the implications of the model-predicted expansion factors by comparing the # of fish over- or under-estimated by the model and the 1.8-expansion factor in reference to the known population estimate (i.e. the HPE) (Figure 6a), and then considered what proportion of the total population that discrepancy represented (Fig 6b). In overall goal of Figure 6 is to see much smaller turqouise bars than gray bars, indicating a smaller discrepancy in the # of fish (and subsequent % of population) predicted by the model versus the 1.8-expansion. As stated above the model out-performs the 1.8 expansion in most cases, and tends to over- or under- estimate in the same direction as the 1.8 expansion factor. It ony produced large discrepancies in the number of fish in a few cases (e.g. Survey #77), and in some cases this did not end up representing much of the population. In cases where the discrepancies were >50% of the population, the number of fish mis-predicted was very small; this is to be expected in small populations as a change in a small number of fish represents a large proportion of the population. 

<br>

```{r echo=F, warning=F, message=F}
ggarrange(dDBE, pDBE, nrow=2)
```

<br>

<br>

<br>

### **Harrison Exclusion** ### 

----------------

The Harrison River is a notoriously difficult system to enumerate. It has had a relatively recent population increase, and accounts for several cases of very high discrepancies between LPE and HPE (Figure 1). It is difficult to count visually, and difficult to enumerate properly via mark-recapture. 

As such, we examined whether the above model fit would be improved if the Harrison River was excluded and it received "special attention". Preliminary and summarized model fit results are shown below, as the aforementioned methods all still apply to this exercise. 

```{r echo=F, include=F}
# Randomly select 80% of data. MUST RUN set.seed(2) first for reproducibility! 
# note addition of term from below - rescaling 'lpe'
set.seed(2)
cal.80.noH <- cal %>% 
  filter(gazetted_stream_name != "Harrison River") %>%
  slice_sample(prop=0.8) %>%
  mutate(hpe = ifelse(is.na(hpe), hpe_escdb, hpe)) %>%
  mutate(index = ifelse(is.na(index), hpe/lpe, index)) %>%
  print()
cal.80.noH.names <- cal.80.noH$usid

# Filter remaining 20% of data for model validation
cal.20.noH <- cal %>%
  filter(gazetted_stream_name != "Harrison River") %>%
  filter(!usid%in%c(cal.80.noH.names)) %>%
  print()

#--------- FIT MODELS
# model suggests re-scaling 'lpe' - see above code chunk 

m.global.noH <- glm(index ~ system_stability + large_woody_debris + water_clarity + substrate_shade + lpe_sc + lpe_method, data=cal.80.noH,  family=Gamma(link="inverse"), na.action = "na.fail")
summary(m.global.noH)

# Run all combinations of models
m.dredge.noH <- dredge(m.global.noH, beta="none", rank="AICc")
summary(m.dredge.noH)


#--------- EVALUATE MODELS
# 1.1. TOP MODEL SET: dAIC<4  
model.sel.dAIC4.noH <- get.models(m.dredge.noH, subset=delta<4)                    # note essentially identical outcome with weight<=0.95
models4.noH <- model.sel(model.sel.dAIC4.noH)
model.table.dAIC4.noH <- subset(m.dredge.noH, delta<4)                             # just model table for different viewing options
# 1.2. MODEL-AVERAGE TOP MODEL SET: dAIC<4
model.avg.dAIC4.noH <- model.avg(model.sel.dAIC4.noH)

# 2. TOP MODEL: dAIC=0
model.sel.top.noH <- get.models(m.dredge.noH, subset=1)
m.top.noH <- glm(index ~ large_woody_debris + lpe_method + lpe_sc + system_stability, data=cal.80.noH, family=Gamma(link="inverse"), na.action="na.fail")

# Model tables for visualizing
model.table.dAIC4.noH <- subset(m.dredge.noH, delta<4)
model.table.top.noH <- subset(m.dredge.noH, subset=1)


#--------- PREDICT/VALIDATE MODEL
# Based on model-averaged parameters: 
predictions.avgmod4.noH <- predict(model.avg.dAIC4.noH, type="link", backtransform=T, full=T, cal.20.noH, se.fit=T)
cal.20.noH$index_avgmod4 <- predictions.avgmod4.noH[[1]]
cal.20.noH$index_avgmod4.se <- predictions.avgmod4.noH[[2]]

predictions.topmod.noH <- predict(m.top.noH, type="link", backtransform=T, full=T, cal.20.noH, se.fit=T)
cal.20.noH$index_topmod <- predictions.topmod.noH[[1]]
cal.20.noH$index_topmod.se <- predictions.topmod.noH[[2]]
```

The top model (dAICc=0) contained terms for LWD, LPE method, LPE, and system_stability (water clarity and substrate_shade were excluded). The top model accounted for 48% of the weight of evidence in the top set of models and had over 2-times the weight of evidence compared to the second and third models (w=48 versis w=0.22 and w=0.21, respectively; Table 4). 

<br>

<font size="2"> *Table 4. Set of top models with dAICc ("delta") < 4 for the dataset excluding the Harrison River.* </font>

```{r echo=F, warning=F, message=F}
# Top models table
knitr::kable(model.table.dAIC4.noH)
#knitr::kable(cbind("Model"=row.names(models4), dplyr::as_data_frame(models4)%>%select(-family)))
```

<br>

From the top model set we were able to calculate Relative Variable Importance (RVI) values as we fit a balanced set of candidate models. In contrast to above, LPE method, LPE and system stability were all the most important (RVI=1), which was 1.4-times as important as large woody debris, and approx. 3 times as important as water clarity (Table 5). It is surprising to have 3 variables tie for complete importance (RVI=1). 

<br>

<font size="2"> *Table 5. Relative variable importance of variables included in the model-averaged set of top models (dAICc < 4) for the dataset excluding the Harrison River.*</font>

```{r echo=F, warning=F, message=F}
# RVI table
knitr::kable(cbind("Variable name"=rownames(as.data.frame(cbind(model.avg.dAIC4.noH$sw))),
                   as.data.frame(cbind(model.avg.dAIC4.noH$sw),row.names=F)%>%rename(`Relative importance`=V1)))
```

```{r echo=F, include=F}
####### Plot background work, not shown ###
# PLOT PREDICTIONS: INDICES AND POPULATION SIZES
# "True" index (hpe/lpe) vs. modelled index - note top model performs poorly compared to model-averaged and is not shown
index.noH<-ggplot() +
  geom_hline(yintercept = 1.8, colour="red") +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=index), stat="identity", colour="black", fill="white") +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=index_avgmod4), stat="identity", fill="blue", alpha=0.8, width=0.7) +
  theme_bw() 

# Implications of "true" population size (HPE) vs. resulting 1.8-expanded HPE and model-expanded HPE
cal.20.noH <- cal.20.noH %>% 
  mutate(pop_est_18 = lpe*1.8,
         pop_est_avgmod4 = lpe*index_avgmod4,
         pop_est_topmod = lpe*index_topmod,
         pop_se_avgmod4 = lpe*index_avgmod4.se,
         pop_se_topmod = lpe*index_topmod.se)

pop_ests.noH<-ggplot() +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=hpe), stat="identity", colour="gray70", fill="gray70", width=0.8) +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=pop_est_18), stat="identity", colour="black", fill="white", alpha=0.5, width=0.6) +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=pop_est_avgmod4), stat="identity", fill="blue", alpha=0.5, width=0.6) +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=pop_est_topmod), stat="identity", fill="green", alpha=0.5, width=0.6) +
  annotate(geom="text", x=20, y=300000, label="Excluding Harrison River") +
  labs(y="Population estimate", x="Survey")


# PLOT PREDICTIONS: DIFFERENCES BETWEEN ESTIMATES
# Only doing top model set as the top model solo didn't perform well. 
DBEs.noH <- cal.20.noH %>%
  dplyr::select(usid, n_row, hpe, pop_est_18, pop_est_avgmod4) %>%
  pivot_longer(cols=c(pop_est_18, pop_est_avgmod4), names_to = "expand_type", values_to = "pop_est") %>%
  mutate(difference=hpe-pop_est,
         pDBE=(difference/hpe)*100) %>%
  group_by(usid) %>%
  mutate(winner = min(abs(difference))) %>%
  mutate(winner = ifelse(winner==difference, "WIN", ifelse(winner*(-1)==difference, "WIN", NA))) %>%
  print()

dDBE.noH<-ggplot(DBEs.noH, aes(x=factor(n_row), y=difference, group=expand_type, fill=expand_type, colour=winner)) +
  geom_hline(yintercept = 0, colour="black") +
  geom_bar(stat="identity", position="dodge", width=0.5) +
  scale_fill_manual(labels = c("lpe*1.8", "lpe*model-averaged predicted index"), values=c("gray70", "turquoise")) +
  scale_colour_manual(labels = c("WIN", NA), values=c("black", NULL), guide=F) +
  labs(y="# over-est        # under-est", fill="expansion method", x="Survey") +
  annotate(geom="text", x=20, y=140000, label="Excluding Harrison River") +
  theme_bw() +
  theme(axis.text = element_text(colour="black"),
        legend.position = c(0.35,0.15),
        legend.background = element_rect(colour="black"),
        legend.key.height = unit(2, "mm"),
        legend.key.width = unit(3, "mm"),
        legend.text = element_text(size=8),
        legend.title = element_text(size=9, face="bold"))

pDBE.noH<-ggplot(DBEs.noH, aes(x=factor(n_row), y=pDBE, group=expand_type, fill=expand_type, colour=winner)) +
  geom_hline(yintercept = 0, colour="black") +
  geom_bar(position="dodge", stat="identity", width=0.5) +
  scale_fill_manual(labels = c("lpe*1.8", "lpe*model-averaged predicted index"), values=c("gray70", "turquoise")) +
  scale_colour_manual(labels = c("WIN", NA), values=c("black", NULL), guide=F) +
  labs(y="% over-est      % under-est", fill="expansion method", x="Survey",
       caption="Fig 8. Discrepancy in a) number of fish and b) proportion of population between the 1.8-expanded population \nestimate and the model-estimated population estimate compared to the known HPE. Excludes the Harrison River. The \nwinning comparison is outlined in black. ") +
  annotate(geom="text", x=15, y=80, label="Excluding Harrison River") +
  theme_bw() +
  theme(axis.text = element_text(colour="black"),
        plot.caption = element_text(hjust=0, face="italic"),
        legend.position = c("none"))

##### 3. SUMMARY STATS: 
# cases where model was best based on # fish (not proportion - but gives the same result in this case)
winners.noH <- DBEs.noH %>%
  filter(winner=='WIN') %>%
  group_by(expand_type) %>%
  summarize(n=n()) %>%
  print()
#######
#######
```

<br>

<span style="text-decoration:underline"> **Model validation** </span>

As above, we predicted expansion factors from both the top model and from model-averaging the set of top models. Based on the two methods of model-derived expansion factors (top model and model-averaged set), model-averaging the top set of models (dAIC < 4) again provided better expanded population estimates compared to the top model, which consistently underestimated the population size (Figure 7). When assessing the differences in population estimate (i.e., number of fish) between the known population estimate and the two methods of expansion factors, the model-averaged predicted expansion factor performed better than the 1.8 expansion factor in `r winners.noH[winners.noH$expand_type=="pop_est_avgmod4",]$n` out of `r sum(winners.noH$n)` cases (`r round((winners.noH[winners.noH$expand_type=="pop_est_avgmod4",]$n/sum(winners.noH$n))*100, 0)`%), i.e., the predicted population estimate was closer to the real population estimate (Figure 4). The model over-estimated the population size for `r pull(DBEs.noH%>%filter(expand_type=="pop_est_avgmod4"&difference<0)%>%ungroup()%>%summarize(n=n()))` surveys and underestimated the population size for `r pull(DBEs.noH%>%filter(expand_type=="pop_est_avgmod4"&difference>0)%>%ungroup()%>%summarize(n=n()))` surveys. Similarly, the 1.8-expansion factor over-estimated for `r pull(DBEs.noH%>%filter(expand_type=="pop_est_18"&difference<0)%>%ungroup()%>%summarize(n=n()))` surveys and underestimated for `r pull(DBEs.noH%>%filter(expand_type=="pop_est_18"&difference>0)%>%ungroup()%>%summarize(n=n()))` surveys. 

```{r echo=F, message=F, warning=F}
ggplot() +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=hpe), stat="identity", fill="gray60",  alpha=0.7) +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=pop_est_18), stat="identity", fill="white", alpha=0.4) +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=pop_est_avgmod4), stat="identity", fill="turquoise", colour="blue", alpha=0.7, width=0.5) +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=pop_est_topmod), stat="identity", fill="red", colour="black", alpha=0.7, width=0.5) +
  
  geom_errorbar(data=cal.20.noH, aes(x=factor(n_row), ymin=pop_est_avgmod4-pop_se_avgmod4, ymax=pop_est_avgmod4+pop_se_avgmod4), 
                stat="identity", colour="blue", width=0.3) +
  geom_errorbar(data=cal.20.noH, aes(x=factor(n_row), ymin=pop_est_topmod-pop_se_topmod, ymax=pop_est_topmod+pop_se_topmod), 
                stat="identity", colour="black", width=0.3) +
  
  #geom_point(data=DBEs%>%group_by(usid)%>%mutate(winner = ifelse(min(difference)==difference, "WIN", ""))%>%filter(winner=='WIN'), 
  #           aes(x=factor(n_row), y=))
  labs(y="Population estimate", x="Survey number", caption="Fig 7. High-precision population estimate (dark gray) compared to the low-precision estimate expanded by 1.8 \nexpansion factor (light gray), the top model predicted expansion factor (red), and the model-averaged predicted \nexpansion factor (turquoise) for 20% of the dataset excluding the Harrison River.") +
  annotate(geom="text", x=15, y=300000, label="Excluding Harrison River") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0, face="italic"),
        axis.text = element_text(colour="black"),
        axis.title = element_text(face="bold"))
```

<br>

We again compared the implications of the model-predicted expansion factors by comparing the # of fish over- or under-estimated by the model and the 1.8-expansion factor in reference to the known population estimate (i.e. the HPE) (Figure 8a), and then considered what proportion of the total population that discrepancy represented (Fig 8b). In overall goal of Figure 8 is to see much smaller turqouise bars than gray bars, indicating a smaller discrepancy in the # of fish (and subsequent % of population) predicted by the model versus the 1.8-expansion. As stated above the model out-performs the 1.8 expansion in most cases, but may tend to over-estimate the population size more frequently (Fig 19b). It ony produced large discrepancies in the number of fish in a few cases (e.g. Survey #77), and in some cases this did not end up representing much of the population. In cases where the discrepancies were >50% of the population, the number of fish mis-predicted was very small; this is to be expected in small populations as a change in a small number of fish represents a large proportion of the population. 

<br>

```{r echo=F, warning=F, message=F}
ggarrange(dDBE.noH, pDBE.noH, nrow=2)
```

<br>

<br>

**All together now** 

Removing the Harrison obviously has implications for model parameter estimates and significance (Fig 9), although there is preliminary evidence that the Harrison-exclusion model-averaged parameters may predict well when there are large expansion factors (Fig 10).

```{r echo=F, warning=F, message=F}
ggplot() +
  geom_errorbarh(data=data.frame(coefTable(model.avg.dAIC4, full=TRUE)), 
                 aes(xmin=Estimate-`Std..Error`, xmax=Estimate+`Std..Error`, 
                     y=rownames(data.frame(coefTable(model.avg.dAIC4, full=TRUE)))), colour="green", height=0.2, size=1) +
  geom_point(data=data.frame(coefTable(model.avg.dAIC4, full=TRUE)), 
             aes(x=Estimate, y=rownames(data.frame(coefTable(model.avg.dAIC4, full=TRUE)))), colour="green", fill="white", shape=21, size=2, stroke=1.1) +
  geom_errorbarh(data=data.frame(coefTable(model.avg.dAIC4.noH, full=TRUE)), 
                 aes(xmin=Estimate-`Std..Error`, xmax=Estimate+`Std..Error`, 
                     y=rownames(data.frame(coefTable(model.avg.dAIC4.noH, full=TRUE)))), colour="blue", height=0.2, size=1) +
  geom_point(data=data.frame(coefTable(model.avg.dAIC4.noH, full=TRUE)), 
             aes(x=Estimate, y=rownames(data.frame(coefTable(model.avg.dAIC4.noH, full=TRUE)))), colour="blue", fill="white", shape=21, size=2, stroke=1.1) +
  geom_vline(xintercept=0, linetype="dashed", colour="red") + 
  labs(y="Predictor variable level", x="Parameter estimate", 
       caption="Fig 9. Model-averaged parameter estimates (+/- SE) for models with (green) and without (blue) the Harrison River.") +
  theme_bw() +
  theme(plot.caption = element_text(face="italic"),
        axis.text = element_text(colour="black"),
        axis.title = element_text(face="bold"))
```

```{r echo=F, warning=F, message=F}
ggplot() +
  geom_hline(yintercept=1.8, colour="red", linetype="dashed") +
  geom_bar(data=cal.20.noH, aes(x=factor(n_row), y=11), stat="identity", colour="transparent", fill="gray70", alpha=0.4) +
  geom_point(data=cal.20, aes(x=factor(n_row), y=index), shape=21, fill="white", size=2, stroke=1.1) +
  geom_errorbar(data=cal.20, aes(x=factor(n_row), ymin=index_avgmod4-index_avgmod4.se, max=index_avgmod4+index_avgmod4.se), col="green", width=0, size=1) +
  geom_point(data=cal.20, aes(x=factor(n_row), y=index_avgmod4), shape=22, fill="white", colour="green", size=2, stroke=1.1) +
  geom_point(data=cal.20.noH, aes(x=factor(n_row), y=index), shape=21, fill="white", size=2, stroke=1.1) +
  geom_errorbar(data=cal.20.noH, aes(x=factor(n_row), ymin=index_avgmod4-index_avgmod4.se, max=index_avgmod4+index_avgmod4.se), col="blue", width=0, size=1) +
  geom_point(data=cal.20.noH, aes(x=factor(n_row), y=index_avgmod4), shape=22, fill="white", colour="blue", size=2, stroke=1.1) +
  labs(y="Index", x="Survey", caption="Fig 10. Observed index (black) compared to model-average predicted index (+/- SE) with (green) and without (blue) \nthe Harrison River.") +
  theme_bw() +
  theme(plot.caption = element_text(hjust=0, face="italic"),
        axis.text = element_text(colour="black"),
        axis.title = element_text(face="bold"))
```

```{r echo=F, warning=F, message=F}
DBEStest <- DBEs %>% mutate(source="with harrison")
DBEStest.noH <- DBEs.noH %>% mutate(source="without harrison")

test<- full_join(DBEStest, DBEStest.noH, by=c("usid", "n_row", "hpe", "expand_type", "pop_est", "difference", "pDBE", "winner", "source"))

ggplot(data=test%>%filter(expand_type=="pop_est_avgmod4"), aes(x=factor(n_row), y=difference, group=source, fill=source)) +
  geom_hline(yintercept = 0, colour="black") +
  geom_bar(stat="identity", position="dodge", colour="black", width=0.6) 
```



<br>

<br>

<br>

<br>

<br>

<br>

<br>





















----------------

## Appendices ##

----------------

Below are sections on the background and methods that went into the above work. As well, some of the content below is additional exploratory work (e.g., exponential models). The big issue is that the dynamic environmental variables are missing a considerable number of data points and are not missing at random. I went through the exercise of examining relationships, VIF/multicollinearity etc. as an informative process, but it ultimately isn't factored in to the model above. 

<br>

<br>

----------------

## **Background and objectives** ##     

----------------
 
The foundation of this work was based on comparing the high-precision to low-precision for each system and year, and examining the range and possible reasons for variability. This was a 10-year study (~2008-2018) funded by the Southern Endowment Fund (Pacific Salmon Commission, reports online), but calibration data has been collected back to the 1980s. Very early calibration work (ca. 1980s) typically focused on small, clear streams with fences. The occasional larger system with a mark-recapture was included. During the SEF years, the database of calibration streams was expanded to include larger systems, sonar programs, and more 'problematic' systems with challenging live counting conditions. The SEF reports limit analyses only to those systems surveyed in the 10 years program. Following the completion of this, a more detailed examination of just the small, clear streams (Early Stuart) reaching back to the 1980s was completed. Through all this work, the primary variable was a 'calibration index', calculated as *I* = *LPE*/*HPE*, where LPE is the low-precision spawner population estimate (from visual counts) and HPE is the high-precision spawner population estimate (from sonar, mark-recapture or fence programs).

Although this ground work provides a key foundation, no previous work has integrated stream characteristics, environmental conditions, escapement, and stream life (spawner replenishment) into one modelling framework to test multiple hypotheses regarding variation in calibration index. Furthermore, early predictive models developed were not validated against a sub-sample of data, quantitative model fits and variable importance were not assessed, no consideration of model-averaging occurred, and no attempts were made at retrospective escapement adjustments with new a calibration index. 

This document outlines work by K. Davidson and some of the rabbit holes to avoid. 

With specific regard to early work, all years and streams, unless wildly unreliable or biased, should be used as they all represent important data points in this data-limited scenario. Classifying and describing error variation, rather than tossing it out willy-nilly is typically recommended when possible (based on discussions with B. Davis).

"Visual count vs. true number spawning there"

<br>

**Temporal evolution of calibration work**

The purpose and context of calibration work has evolved over time. Early entries were compiled opportunistically when a high precision estimate (HPE) and a low-precision estimate (LPE) both occurred on same system-year. Around 2008, StAD staff undertook a 10-year calibration project to expand the spatial extent of systems used for calibration to include larger streams across a variation of conditions. While the ultimate goal of both eras of calibration was the same, recent expansion to include large, "abnormal" streams artificially gives the impression that visual survey quality has declined (and the index subsequently increased) over time (Fig 1). Likewise, early calibration data points focus on Early Stuart stream fence and live counts; fence counts have high certainty, and live counts on small creeks are often more accurate. Unfortunately, recent low escapements on the Early Stuart systems preclude the ability to assess long-term intra-stream count variability.

```{r, echo=F, warning=F, message=F, include=F}
all<-ggplot(cal, aes(x=year, y=index, fill=lpe_method, size=hpe)) +
  annotate(geom="text", x=1988, y=16, label="A") +
  geom_text(data=cal%>%filter(index>6), aes(label=sid), size=3, hjust=1.08) +
  geom_point(shape=21, colour="black", alpha=0.75) +
  scale_x_continuous(breaks=seq(1988,2020, by=3)) +
  scale_y_continuous(breaks=seq(0,18,by=2)) +
  scale_size_continuous(range = c(2, 7)) +
  labs(x="Year", y="Index", fill="LPE method", size="Run size") +
  theme_bw() +
  theme(legend.margin=margin(t=0.1, r=0.1, b=0.1, l=0.1, unit="cm"),
    legend.spacing.x = unit(0.1, "mm"),
    legend.spacing.y = unit(0.1, "mm"),
    legend.background = element_rect(colour="black"),
    legend.position=c(0.1,0.70),
    legend.text = element_text(size=9),
    legend.title = element_text(size=11),
    legend.key.size = unit(0.5, "cm")) +
  guides(fill=guide_legend(ncol=2)) 

paired_obs <- cal %>% group_by(sid) %>% summarize(n=n()) %>% filter(n==2) %>% pull(sid)

paired<-ggplot(data=cal%>%filter(sid%in%paired_obs, !is.na(hpe)), aes(x=year, y=index, fill=lpe_method, size=hpe)) +
  annotate(geom="text", x=2007, y=16, label="B") +
  geom_text(data=cal%>%filter(sid%in%paired_obs, index>6), aes(label=sid), size=3, hjust=1.08) +
  geom_point(shape=21, colour="black", alpha=0.4) +
  scale_x_continuous(breaks=seq(2007,2020, by=2)) +
  scale_y_continuous(breaks=seq(0,18,by=2)) +
  scale_size_continuous(range = c(3, 5)) +
  labs(x="Year", y="Index", fill="LPE method") +
  theme_bw() +
  theme(legend.position="none") 
```

```{r echo=F, warning=F, message=F}
ggarrange(all, paired, nrow=2)
```

*Fig 1. a) Calibration index over time (calculated as HPE/LPE) for all systems. b) Calibration index over time for systems with paired ground and aerial visual counts within the same year. Size of points scaled by run size (as given by the high-precision estimate).*

<br>

<br> 

<br>

----------------

## **Methods: Data collection and collation** ##    

----------------

### Rules for deriving estimates ###

There has been some inconsistencies in how specific numbers for peak live, carcasses, fence counts, etc. have been collected over the course of the calibration program. In November 2020 and January 2021, S. Decker and myself discussed in depth the objective and scientific question behind the future trajectory of the calibration analysis. Given a lack of clarity and some inconsistencies over time, the following guidelines were used when collecting historical data. These details are especially important as it will likely explain why calibration data points do not match escapement estimates in the Sockeye Near Final escapement database, on NuSEDs, etc.  

Up to now, the objective seems to have been to calibrate human eyeballs to machine eyeballs; thus visual counts below sonars/fences were excluded so that the counts are comparable. However, it is important to note that this means one method is influencing how another method collects data. This is not what would be done if a system was purely roving - the live count would not stop at a fence, because the fence in question would not exist. The problem is further intensified when repeated LPEs are gathered from the same stream-year, but rely on non-independent counts (e.g., ground carcasses applied to aerial counts and vice-versa). Thus the inference from this calibration set is to see what would happen if a HPE method 'fell apart' mid-season, and a visual count was the only backup (e.g., sonar weir or fence blew out). Extrapolation to systems outside of this data set, or assumptions about converting a HPE system to a roving program (i.e., LPE), are not appropriate conclusions to draw from this work. 

<br> 

**High-precision data**    
If a high-precision estimate (HPE) was from a:

* Fence count: 'raw' live through the fence is used. Typically no extrapolation or infilling is done on fences, but occasionally breached fences result in infilling (e.g., Bowron fence 1995). Removals to tributaries or other systems are made (e.g., Nadina removals at Stellako) to the best of ability, *but note these are based on visual counts*. No visual counts below the fence are included. Fecundities are typically taken from the fence and manually added to the final population estimate; likewise, these fecundities are manually added to the raw fence count as they would have been part of the absolute system count (and to make inter-year comparisons more consistent, given that some years no samples are collected and other years up to 100+ are collected). Note specific decisions about how to parse out Stellako and Nadina were difficult, and varied annualy depending on method (travel time, tagging, SONAR counts, DNA, scales). It is likely the Stellako HPEs may need refining, but early years especially are data-limited. For now, the Stellako estimates are the best, highest-precision estimates **given the data available.**
* Sonar: 'raw' count past sonar, plus tail-extrapolation and/or infilling, is used. Removals to tributaries or other systems are made (e.g., Nadina removals at Stellako, channel removals) to the best of ability, *but again note these are based on visual counts*. No visual counts below the fence are included. Fecundities are usually already counted past sonars (as opposed to taken downstream of a sonar weir) so no manual additions are required. 
* Mark-recapture: total population estimate minus removals to tributaries (which is done prior to the final calculation). Fecundities are included. <span style="color: red;">**An outstanding issue is how to deal with broodstock removals.**</span>

Note though that the final escapement estimate (which is compiled irrespective of this calibration work) for fences and SONARs, where there is a "hard boundary", would be created by adding the 'live counted through fence/sonar' + 'live count of spawners below fence' * 'expansion' for a more representative escapement estimate of the whole system. However, as noted above, the objective of this calibration exercise is to estimate the total number of spawners in the system *given data limitations*, simulating what might happen if a sonar or fence blew out and a visual count was all that was able to be used. 
Therefore, the HPE given in 'calibration_2020_KDupdate.xlsx' may not match the Near Final escapement estimate in 'DFO Sockeye Escapement All Years (_date_).xlsx', NuSEDs, etc. 

Any high-precision estimated deemed or suspected to be seriously biased (e.g., mark-recapture) should not be included in the calibration database.

<br>

**Low-precision data**    
The decisions around low-precision data, and how they correspond to the HP data and framework of the study objectives, has been much murkier. It is also much more difficult to tease these out of historical records as they were not intended to be the primary method of enumeration. They often do not receive the same level of scrutiny that high-precision estimates do, but best attempts have been made. 

If an LPE is from an:

* Aerial survey: aerial live count plus aerial unsexed dead.
    + Previous thoughts were that an exception to aerial unsexed dead could be made in system-years where carcasses either couldn't be counted (too many fish) or because extensive ground recovery efforts preclude the ability to distinguish chopped/unchopped across areas. In this case, ground recoveries could be added to the aerial count for a more robust LPE. **However**, the implications of adding ground carcasses to aerial surveys (and vice-versa) create issues around non-independence of observations. Furthermore, carcass recovery efforts are vastly different depending on the HPE method (MR vs. SONAR). Therefore, if carcasses were not counted from the helicopter at the same time as the aerial live count, there are no carcasses added. 
    + It is rare to have multiple aerial surveys each system-year, but if data are available and it is of interest, cumulative aerial unsexed dead carcasses could also be used. This is just an idea and has not been pursued in the current analysis.
* Ground survey (boat, raft or walk): Peak live ground count, plus cumulative ground carcass counts, ideally up to and including the day of the live count, but sometimes carcass recoveries are behind (up to several days). The ground LPE should not include any carcasses viewed from the aircraft in any aerial surveys leading up to the peak live ground count date. It is extremely rare that unsexed carcasses are counted in ground surveys (recovered yes, but not usually unsex dead count); to my knowlege this was only available for one system-year, as such it was not considered as carcasses contributing to the LPE. **Note that carcasses are not currently added to visual counts performed on MR-years as carcass recovery effort is abnormally high.**

This is especially important to determine pre-analysis because some system recieved both ground and aerial counts in the same year for extra calibration. Therefore it is important to establish ground and aerial counts as independent so that both can be used in each system-year. 

<br>

Other specific system-year notes are made in 'calibration_2020_KDupdate.xlsx'.

<br>

**LPEs and HPEs**    
Examining relationships between different estimate types and the index (LPE/HPE) they produce. At low escapements the relationship between both the HPE and LPE and HPE and Index is tighter; as escapement increases so does variability (Fig 2), but when assessing univariate relationships the LPE explains 87% of the variation in HPE alone. 

```{r, warning=F, echo=F, include=F}
hpe_lpe_plot <- function(y_var, x_var, ann_x, ann_y, ann_lab, y_lab, x_lab){
  ggplot(cal, aes(x=.data[[x_var]], y=.data[[y_var]])) +
    annotate("text", x=ann_x, y=ann_y, label=ann_lab) +
    geom_point(shape=21, fill="blue", colour="black", stroke=1.2, size=2, alpha=0.8) +
    geom_smooth(colour="red", method="lm", se=F) +
    labs(x=x_lab, y=y_lab) +
    theme_bw() +
    theme(legend.position = "bottom") +
    stat_cor(aes(label=..rr.label..), size=3, label.y=ann_y-(ann_y/10), label.x=ann_x-(ann_x/10), show.legend=F, r.accuracy=0.01) +
    stat_cor(aes(label=..p.label..), size=3, label.y=ann_y-(ann_y/5), label.x=ann_x-(ann_x/7), show.legend=F, p.accuracy=0.001)
}
```

```{r, echo=F, warning=F, message=F}
ggarrange(hpe_lpe_plot("hpe", "lpe", 0, 600000, "A", "HPE", "LPE"), 
          hpe_lpe_plot("index", "hpe", 0, 17, "B", "Index", "HPE"), 
          hpe_lpe_plot("index", "lpe", 250000, 17, "C", "Index", "LPE"), ncol=2, nrow=2)
```

*Fig 2. Relationships between HPEs and a) corresponding LPEs (no expansion applied) and b) index calculated from LPE/HPE.*

```{r, echo=F, include=F, warning=F, message=F}
# Examining how much variation is explained by LPE alone 
lm_hpe_lpe <- lm(hpe ~ lpe, data=cal)
summary(lm_hpe_lpe)
r_hl <- resid(lm_hpe_lpe)
plot(r_hl)
hist(r_hl)
qqnorm(r_hl)
qqline(r_hl)
```

<br> 

<span style="text-decoration:underline"> A note on environmentals </span>    

The plots in the appendix highlight previous patterns between escapement and environmental factors unearthed by P. Welch and others. It is important to note the difference between dynamic and static environemntal covariates collected by StAD through time. All streams can be qualitatively classified given their size, stability, water clarity, substrate colour, canopy cover, and presence of large woody debris (LWD) relative to each other. These traits do not change annually and highlight broad differences among study systems (e.g., the size of Gluske vs. Stellako, the clarity of Tachie vs. Little Rivers, etc.); these are static environmentals, they do not represent conditions on the day of the survey. 
In contrast, we record dynamic environmentals that are indicative of environmental conditions in 'real-time'. These include water level/flow, bankfull, brightness, cloud cover, fish visibility, water clarity (estimated depth able to see), any mention of other species affecting visual survey ability, and more recently observer efficiency (OE), a measure of the proportion of fish you think you can observe given all the environmental variables impacting visibility. 

While dynamic environmentals are ideal, the data recording quality has changed substantially since the beginning of the calibration work (1988). Eary programs barely recorded any environmentals above "vis good", and OE (the ideal metric) only began around 2012. Static environmentals will be included in global models to account for some of these missing gaps, but continued calibration work *must* record environmentals consistently for future model development or retrospectives.

Some existing relationships between escapement index and environmental variables are given in the 'Data Exploration' section below. Some environmental variables collected will be less useful. For example, while canopy cover might in theory influence an aerial flight count, flights are typically not used on systems with high canopy cover due to that issue. Furthermore, canopy cover and LWD can change among years due to wildfires, requiring re-evaluation of static variables (which puts them in the realm of dynamic variables...)

<br> 

One consideration is how to extrapolate results to streams outside of the calibration database. A complete inventory of streams and their qualities (whatever ends up being the important predictors from GLMM exercises) could be compiled for quick application, or they can be fed directly into the model to estimate their index. 

<br>

<br>

----------------

## **Methods: Model development and considerations** ##

----------------

### Bayesian/machine-learning models ###

**State-Space Models (SSM)**    
While these, in practice, seem ideal for our calibration dataset as they can model both process and observation error in a time series, it is highly questionable as to whether they would be appropriate given the sparse time series across many systems. The main issue that arises is the assumption that an estimate at time *t* is independent, apart from dependence on time *t-1*, and that past time-steps are used to make inference about subsequent time steps. The book-keeping process of SSMs is the information gleaned over time. As we do not have this for each system, and you cannot use the Adams River in 2010 to make inference about Chilko in 2011, unfortunately SSMs appear to be unrealistic for the dataset at the present. Further example; HPEs are only obtained for some systems on dominant years, but not always reflective of brood cycles/primary age cohorts.

<br>

**Boosted Regression Trees (BRT)**   
Considered, but as with SSMs, a lot of data are required and it is typically a model used for a time series as the machine-learning patterns are used to train the model over time. As with SSMs, this should be considered for future analyses if sample size/temporal resolution increase.

<br>

### Traditional models ###

Since SSMs and BRTs are out, the final model form is currently unknown (e.g., GLM, GAM, etc.). However, the theoretical framework of SSMs, i.e., the need to account for, or at least separate, both process variation and observation error, still holds. One possible way of doing this is might be to incorporate covariates that account for these changes. For example, a simple model: 

Index ~ quantity of fish (LPE) + LPE_method + water quantity vars + water quality vars + stream characteristics + *additional species* + (1|year)

Recall that there are two levels of covariate measurement: system and system-year. 

<br>

**Enumeration covariates**

* LPE: The better predictor would likely be HPE, but in legitimate, non-calibration years, this wouldn't be available. Therefore use LPE.   
  + or, Spawner density: density of spawners, or available spawn habitat. Could potentially replace stream_size, although these capture slightly different aspects. Stream_Size accounts for difficulties counting large bodies of water well, and the potential to miss/double count fish; density accounts for large runs packed into small amount of spawn habitat that are difficult to count, particularly if stacked vertically. **However may be difficult to estimate given current state of data recording**. 
* LPE_method: differences between aerial and ground surveys have been noted for different systems. This analysis needs to be re-confirmed. 

<br>

**Fixed environmental covariates**    
Note these will be the bottleneck and likely be the cause for reduction of the calibration dataset, particularly water quantity. 
Water quantity. To note, should discharge (or some quantitative water feature, e.g., level) be used, consider calculating it as a % deviance from historical mean. This could better capture high water events that impact visual counting, although could eliminate the relative difference among streams (e.g., how the Harrison is way bigger than Gluske). Perhaps both should be included?

* water quantity: either discharge, level, bankfull, or some measure in-season of water quantity    
* water quality: either fish vis, water vis (in-season) or broad designations ('static', e.g., turbid/tannic, clear, etc.) 
* stream characteristics: static variable of relative stream size to account for EStu tribs vs. Stellako (as example). **Right now this exists as categorical/factor data, could be made more robust by extracting approximate wetted widths from Google Earth, especially if historical images exist (ability to detect/test if major historical changes have occurred over time).**. Also includes LWD, canopy cover, etc.    
* intraspp: likely binary dummy variable noting if a stream has a significant presence of other confounding species that would affect counts (e.g., Kokanee, pink years, Chum on Harrison, etc.).    
* Tied into these are also survey data such as OE. Unfortunately OE only exists for a small sub-set, but consider re-running subset of models with OE covar to test its predictive ability relative to other in-season variables. 

<br>

**Random effects**

* year: accounts for other annual variables - changes to landscapes (e.g., wildfire removing shadows), undergrowth change over time, log jams, etc. that we can't quantify. Also potential changes over time to runs and systems (e.g., run size/cyclicity, spawn timing, residence time, or sex ratios not explicitly captured by LPE).

<br>

**Other covariates**

* While substrate darkness, woody debris, number of pools, overhanging bank veg, etc. are likely natural influences on visual counts, these cannot be quantified each year. It is possible a random effect, or ordinate all-encompassing static system complexity variable can be created to amalgamate multiple variables into one (even just an "ease of counting" variable, where all staff rank systems).
* <span style="color: red;">Residence time - Large implications for visual count as most only get 1 flight (or at least, if 2 flights, the 2nd is typically due to missing POS on the first one, and one point estimate is still used). Consider this more - how to incorporate?</span>    

<br>

<br>

<br>











### Appendix 1: Missing values (Step 0) ###

----------------

Unfortunately, not all covariates are available for all years/systems of data (Figure 6). While the static environmental variables inherently have no missing values, the dynamic (and likely more representative) environmental variables have many missing values, often confounded with time. For example, OE was only recorded from 2012 onward; after 2012 OE values are MCAR, while prior to 2012 they are MNAR (Fig 6).

An overview of missing data shows that only n=27 observations have a full set of observations for the dynamic environmental covariates, spanning from 2009-2018. While some of these are related, it is difficult to infer/infill missing values as they are specific to stream types, water conditions, etc. 

```{r echo=F, warning=F, mesage=F, include=F}
# Create summary df of all environmental covariates available and a system-year grouping variable
## with the exception of water level - discharge is derived from level so they are basically exactly the same, and there are more flow records than level. 
covar.table <- cal %>% 
  select(usid, year, system_stability:large_woody_debris, water_discharge, bankfull:OE, lpe_method, lpe) %>% 
  print()

#------- ALL ENVIRO COVARS
# Drop all rows with a missing value in any of the covariates as above (i.e., only complete data retained)
covar.drop <- covar.table %>%
  drop_na()%>%
  print()
# n=27 full datasets 

# Summarize where missing values occur for plotting
covar.miss <- covar.table %>%
  group_by(year, usid) %>% 
  miss_var_summary(add_cumsum=T) %>%
  mutate(group = ifelse(variable%in%c("system_stability","size","water_clarity","substrate_shade","canopy_cover","large_woody_debris"), "Static", ifelse(variable%in%c("bankfull", "brightness", "cloud_cover", "fish_vis", "water_vis", "OE", "water_discharge", "lpe_method", "lpe"), "Dynamic", "Other"))) %>%
  print()
covar.miss$variable <- factor(covar.miss$variable, levels=c("system_stability", "size", "water_clarity", "substrate_shade", "canopy_cover", "large_woody_debris", "lpe_method", "lpe", "bankfull", "brightness", "cloud_cover", "fish_vis", "water_vis", "OE", "water_discharge"))
```

```{r echo=F, warning=F, message=F}
ggplot() +
  geom_point(data=covar.miss%>%filter(n_miss==0, group!="Other"), aes(x=variable, y=reorder(usid, -year), colour=group), alpha=0.3) +
  labs(y="survey", x="covariates") +
  theme_bw() +
  theme(axis.text.y = element_text(size=5))
```

<font size="2"> *Fig A1.1. All static (blue) and dynamic (pink) variables available for each survey. Points indicate observations, spaces indicate missing observations. Surveys given in order of year.* </font>

It should be noted that while Figure A1.1 displays *all* variables, some are not appropriate for use. Canopy cover (and possibly LWD) are not truly static covariates; while some systems are inherently more woody/bushy than others, they can change annually and rapidly with forest fires, bank sloughing, flow levels, etc. A prime example is Nadina River before and after complete burns from forest fires. We do not currently have the temporal or spatial resolution in the data to capture such fine-scale landscape change effects on calibration indices. 

<br>

<span style="text-decoration:underline"> **Missing values for dynamic variables** </span>

No concern lies with missing values of static predictor variables, only dynamic ones. Further examination of only missing dynamic variable values using the *finalfit* package confirms values are not always missing at random. For any given variable, we are missing anywhere from 19-75% of observations (Table not shown). Note that some indices are currently missing due to outstanding errors to confirm. Year is included as it will be used as a random effect. 

```{r echo=F, include=F}
explanatory = c("year", "bankfull", "brightness", "cloud_cover", "fish_vis",  "water_vis", "OE", "water_discharge")
dependent = "index"

# Table A1.1. Summary of missing values for dynamic variables. 
cal %>% ff_glimpse(dependent, explanatory)
```

```{r echo=F, message=F, warning=F}
# Clearer visual summary of missing values. 
cal %>% missing_plot(dependent, explanatory)
```

<font size="2"> *Fig A1.2. Complete (navy) and missing (light blue) values for dynamic predictor variables. Observation is equivalent to individual surveys, which are roughly ordered by year based on Excel data entry (from 1988 to 2020).* </font>

<br>


```{r echo=F, message=F, warning=F, include=F}
# NA pattern figure; not clear how to interpret but shows clear patterns. 
cal %>% missing_pattern(dependent, explanatory)
# hypothesis testing table with p-values; not clear how to interpret
cal %>% 
  summary_factorlist(dependent, explanatory, na_include=T, p=T)    # doesn't change if na_include=F
```

We can see further clear interactions between missing values and the "significant" effect (untested) of year in almost all cases except for water discharge and index (Figure A1.3 first column). It is also clear that missing cases in any of the 'wetnote' variables (bankfull, brightness, cloud cover, fish/water vis) are clearly related to NA values in any of those given variables; this makes inherent sense as it is most likely that if one variable was missed, they were all missed (i.e., wetnote environmentals were not completed). The initiation of consistently recording the full suite of "modern" wetnote environmentals was probably fairly recently (early 2000s), as evident by the break between box plots in the 'year' column relative to missing values in those variables (rows 2 to 6) (Figure A1.3). 

```{r echo=F, warning=F, message=F}
cal %>% missing_pairs(explanatory, dependent, position="fill")
```

<font size="2"> *Fig A1.3. Matrix relationships between missing cases in each variable (indexed along the right-hand side vertical boxes) and the entire suite of observations for each variable (indexed along the top row of boxes). Gray indicates missing observations. Bars for categorical variables are represented as proportions of the total number of observations relating to missing values; e.g., 100% of the missing bankfull observations are related to 100% of the NA values in bankfull.* </font>

<br>

<span style="text-decoration:underline"> **A note on covariate selection** </span>

While this is an iterative process that requires the following two steps to be conducted simultaneously, there are a few things that can inform prior covariate selection (i.e., *a priori* biological knowledge), given that we have a limited number of observations and therefore need to pare down variable selection. Moving forward, if all variables are going to be considered, the following variables could be **removed** for the given reasons, but variable removal still needs to be tested with appropriate metrics (e.g., VIF and/or pairs plots).

* Canopy cover - for reasons given above    
* System stability - can be captured in bankfull, discharge, etc. and is not a particularly useful metric   
* Substrate shade - extremely arbitrary and can be impacted by other features and change. Also highly confounded with turbidity, and not overly informative   
* Water clarity - while it could be useful, it is currently inconsistently attributed to various systems    
* Fish vis/water vis/water clarity - likely all correlated    
* Cloud cover/brightness - likely one of these could be dropped as they are likely correlated   

**Moving forward, the only variable that should be excluded up front based on knowledge is canopy cover.**

<br>

<span style="text-decoration:underline"> **Addressing missing values** </span>

As stated, there is not much that can be done about missing dynamic variable values. They are representative of the real-time system features and not always easily infilled. A short-term alternative is to exclude all variables with missing values (i.e., dynamic variables) and focus on static variables. The following pre-modelling procedure will explore 1) all variables without addressing missing values, and 2) only static variables.

<br>

<br>

### Appendix 2: Data exploration (Step 1 Zuur) ### 

----------------

*A.2.1 Step 1: Outliers in Zuur et al 2009.  Cleveland dotchart, box/bar plots of response and explanatory variables etc.*

```{r echo=F, include=F}
# FUNCTIONS FOR THIS SECTION

#-------- Outlier plot functions 
# Dot chart function (continuous variables or re-coded categorical variables)
dotchart_fx <- function(x_var, y_var, point_colour){
  ggplot(cal%>%filter(!is.na({{x_var}})), aes(x={{x_var}}, y={{y_var}})) +
    geom_point(colour=point_colour, alpha=0.5) +
    scale_x_continuous(breaks=pretty_breaks()) +
    labs(y="survey #")
}

# Bar chart function (categorical variables)
barchart_fx <- function(x_var){
  ggplot(data=cal%>%
           filter(!is.na({{x_var}}), !grepl("CANT", {{x_var}})) %>%
           group_by({{x_var}}) %>%
           summarize(n_obs=n()), 
         aes(x={{x_var}}, y=n_obs)) +
    geom_bar(stat="identity", colour="black", fill="gray20")
}
```

Static variables:    

* System stability   
* Size    
* Water clarity   
* Substrate shade    
* Canopy cover (removed)    
* Large woody debris    

Dynamic variables:    

* Bankfull     
* Brightness    
* Cloud cover   
* Fish visibility   
* Water visibility    
* OE  
* Water discharge   
* LPE method    
* LPE   

```{r echo=F, warning=F, message=F}
# static vars
ggarrange(dotchart_fx(index, n_row, "red"),
    ggarrange(dotchart_fx(substrate_shade_recode, n_row, "gray20"),
              dotchart_fx(lwd_recode, n_row, "gray20"),
              dotchart_fx(canopy_cover_recode, n_row, "gray20"), ncol=3),
      ggarrange(dotchart_fx(water_clarity_recode, n_row, "gray20"),   # static \/
                dotchart_fx(size_recode, n_row, "gray20"),
                barchart_fx(system_stability), ncol=3), nrow=3)

```

```{r echo=F, warning=F, message=F}
# dynamic vars
ggarrange(dotchart_fx(bankfull_recode, n_row, "gray20"),         
          dotchart_fx(brightness_recode, n_row, "gray20"),
          dotchart_fx(cc_recode, n_row, "gray20"),
          dotchart_fx(fish_vis_recode, n_row, "gray20"),
          dotchart_fx(water_vis_recode, n_row, "gray20"),
          dotchart_fx(OE, n_row, "gray20"),
          dotchart_fx(water_discharge, n_row, "gray20"),
          dotchart_fx(lpe, n_row, "gray20"),
          barchart_fx(lpe_method), ncol=3, nrow=3)
```

<font size="2"> *Fig A2.1. Data exploration of response (index, red) and predictor (black) DYNAMIC variables to be considered for calibration. Factors have been re-coded where appropriate as integer values, where 1 is the smallest/lowest factor value and values increase as factor level values increase. For example, bankful=1 is 0-25% and bankfull=4 is 75-100%. Water vis=1 is 0.25-0.5 and water vis=4 is 3.0-bottom.* </font>

<br>

Some outliers exist in the dynamic variables. For example, one data point of discharge > 1000 cms. In addition, some variables have unbalanced clusters of observations (e.g., water visibility, water clarity, size, brightness; Fig A2.1). Recall too that some of these values are MNAR (see Appendix 1 above). 

<br> 

<br>

<br>

### Appendix 3: Multicollinearity (Step 2 Zuur) ### 

----------------

*A.2.2 Step 2: Multicollinearity in Zuur et al 2009.*

List of variables of interest to examine for collinearity: 

* Size    
* System stability  
* LWD   
* Substrate shade   
* Water clarity   
* (Canopy cover included here just to show relationship)    
* Bankfull     
* Brightness    
* Cloud cover  
* Fish visibility  
* Water visibility    
* OE  
* Water discharge   
* LPE method    
* LPE        

```{r echo=F, include=F}
# Pairs COR - no lpe_method due to categorical
z <- cbind(cal$index, cal$system_stability, cal$size_recode, cal$water_clarity_recode, cal$canopy_cover_recode, cal$lwd_recode, cal$substrate_shade_recode, cal$bankfull_recode, cal$brightness_recode, cal$cc_recode, cal$fish_vis_recode, cal$water_vis_recode, cal$OE, cal$water_discharge, cal$lpe, cal$lpe_method) 

colnames(z) <- c("index", "stab", "size", "clarity", "canopy", "LWD", "sub_sh", 
                 "bankfull", "bright", "cloud", "fish_vis", "water_vis", "OE", 
                 "flow", "lpe", "lpe_meth")
```

Examination of continuous variables indicates collinearity between some variables (defined roughly as *r*>0.6; Figure A3.1). In the initial global model, VIF scores are exceptionally high for several variables (Table 1a). Step-wise variable removal based on highest VIF values indicates that the best combination of explanatory variables to reduce multicollinearity moving forward is: OE, discharge, water visibility, LPE, and LPE method; i.e., removing bankfull and stream size (Table A3.1). 

```{r echo=F, warning=F, message=F}
pairs(z, lower.panel=panel.smooth, upper.panel=panel.cor.fx, diag.panel=panel.hist.fx, cex=1, pch=16)
```

<font size="2"> *Fig A3.1. Pairs plot for variables indicating level of collinearity between each variable pairing. Histograms of data observations given on the diagonal. Smoothed lines (red) generated using default loess smoother. No data transformations. Ordinal categorical variables recoded as integers for purposes of plotting.* </font>

<br>

Quantitative assessment of multicollinearity between all variables (including categorical) is presented in the following Variance Inflation Factor (VIF) tables (Table A3.1a). Step-wise removal of collinear variables results in the remaining 9 variables: system_stability, substrate_shade_recode, water_clarity_recode, bankfull_recode, brightness_recode, water_vis_recode, OE, lpe, lpe_method. Note that this is likely the maximum number of predictor variables that could be used in a multiple regression model, without random effects. The final model may need further reduced set of coviartes to avoid over-fitting.

```{r include=F, echo=F}
#--------- VIF tests - stepwise removal of highly collinear predictors (cutoff VIF>3)

# global
vif.test.1 <- lm(index ~ system_stability + lwd_recode + substrate_shade_recode + size_recode + water_clarity_recode + bankfull_recode + brightness_recode + cc_recode + fish_vis_recode + water_vis_recode + OE + water_discharge + lpe + lpe_method, data=cal) 

# no fish_vis
#vif.test.2 <- lm(index ~ system_stability + lwd_recode + substrate_shade_recode + size_recode + water_clarity_recode + bankfull_recode + brightness_recode + cc_recode + water_vis_recode + OE + water_discharge + lpe + lpe_method, data=cal)  

# no fish_vis, no LWD
#vif.test.3 <- lm(index ~ system_stability + substrate_shade_recode + size_recode + water_clarity_recode + bankfull_recode + brightness_recode + cc_recode + water_vis_recode + OE + water_discharge + lpe + lpe_method, data=cal) 

# no fish_vis, no LWD, no cloud_cover
#vif.test.4 <- lm(index ~ system_stability + substrate_shade_recode + size_recode + water_clarity_recode + bankfull_recode + brightness_recode + water_vis_recode + OE + water_discharge + lpe + lpe_method, data=cal)

# no fish_vis, no LWD, no cloud_cover, no size
#vif.test.5 <- lm(index ~ system_stability + substrate_shade_recode + water_clarity_recode + bankfull_recode + brightness_recode + water_vis_recode + OE + water_discharge + lpe + lpe_method, data=cal)

# no fish_vis, no LWD, no cloud_cover, no discharge
#vif.test.6 <- lm(index ~ system_stability + substrate_shade_recode + water_clarity_recode + bankfull_recode + brightness_recode + water_vis_recode + OE + lpe + lpe_method, data=cal)
```

<br>

<font size="2"> *Table A3.1. Variance Inflation Factor (VIF) scores for various iterations of explanatory variable selection. Note Table 2a shows the VIF scores for the global model; subsequent Tables 1b-f are not shown here but reveal VIF scores for step-wise removal of the variable with the highest VIF score each time.* </font>

```{r echo=F, warning=F, message=F}
knitr::kable(cbind("Table"=c("A3.1a", "(global)", rep("",12)), dplyr::as_data_frame(vif(vif.test.1), rownames = "Variable")))
#knitr::kable(cbind("Table"=c("2b", "(- fish_vis)", rep("",11)), dplyr::as_data_frame(vif(vif.test.2), rownames = "Variable")))
#knitr::kable(cbind("Table"=c("2c", "(- fish_vis)", "(- LWD)", rep("", 9)), dplyr::as_data_frame(vif(vif.test.3), rownames = "Variable")))
#knitr::kable(cbind("Table"=c("2d", "(- fish_vis)", "(- LWD)", "(- cloud)", rep("", 7)), dplyr::as_data_frame(vif(vif.test.4), rownames = "Variable")))
#knitr::kable(cbind("Table"=c("2e", "(- fish_vis)", "(- LWD)", "(- cloud)", "(-size)", rep("", 5)), dplyr::as_data_frame(vif(vif.test.5), rownames = "Variable")))
#knitr::kable(cbind("Table"=c("2f", "(- fish_vis)", "(- LWD)", "(- cloud)", "(-size)", "(-flow)", rep("", 3)), dplyr::as_data_frame(vif(vif.test.6), rownames = "Variable")))
```

<br>

<br>

### Appendix 4: Relationships between response and explanatory variables (Step 3 Zuur) ### 

----------------

*A.2.3 Step 3: Relationships in Zuur et al 2009.* 

List of variables of interest to examine relationships (based on results of step 2 above):

* System stability    
* Substrate shade   
* Water clarity   
* Bankfull    
* Brightness    
* Water visibility    
* OE    
* LPE   
* LPE method    

```{r echo=F, warning=F, message=F}
ggarrange(relation.fx("system_stability", 1, 17, "", "Stability"),
          relation.fx("substrate_shade_recode", 1, 17, "", "SUbstrate shade"),
          relation.fx("water_clarity_recode", 1, 17, "", "Water clarity (static)"), 
          relation.fx("bankfull_recode", 1, 17, "", "Bankfull (%)"),
          relation.fx("brightness_recode", 1, 17, "", "Brightness"),
          relation.fx("water_vis_recode", 1, 17, "", "Water visibility (dynamic)"),
          relation.fx("OE", 25, 17, "", "Observer efficiency (%)"), 
          relation.fx("lpe", 25, 15, "", "LPE"),
          relation.fx("lpe_method", "Aerial", 17, "", "LPE method"), common.legend = T)
```

<font size="2"> *Fig A4.1. Relationships between response variable (index) and explanatory variables (selected during step-wise VIF process) expressed using loess (blue) and cubic spline (red) smoothers, where possible. Note some smoothers require a minimum number of x-axis observations to created informed smoothers. Ordinal categorical variables were recoded as integers for purposes of smoothing.* </font>

<br>

<br>

### Appendix 5: Univariate predictive models ###    

----------------

Two examples of univariate models were created as an exercise to investigate whether single dynamic variables could better (or closely) produce expansion factors as well as the multivariate model. The two variables examined here briefly, OE and discharge ('flow') were chosen due to their promising linear relationship with indices. Out of all of the dynamic environmental variables, they have the least number of missing data points. 

<br>

#### **Index ~ OE** ####    

The first model fit an exponential model as index~OE. It was fit using the *nls()* function. Initial parameters (alpha, beta, theta) were estimated using a linear model and data values. As above, the model was fit to 80% of the data, while 20% were reserved for model validation (sub-samples chosen randomly).

```{r eval=F}
Model formula:
model.exp.oe <- nls(index ~ alpha*exp(beta*OE) + theta , data=exp.test.oe.80, start=start)
```

```{r echo=F, include=F}
exp.test.oe <- cal %>% 
  filter(!is.na(index) & !is.na(OE)) %>% 
  print()

set.seed(123)
exp.test.oe.80 <- exp.test.oe %>% 
  slice_sample(prop=0.8) %>%
  print()
exp.names.oe <- exp.test.oe.80$usid

exp.test.oe.20 <- exp.test.oe %>%
  filter(!usid%in%c(exp.names.oe)) %>%
  print()


# Select an approximate $\theta$, since theta must be lower than min(y), and greater than zero
theta.0 <- min(exp.test.oe.80$index)*0.5  

# Estimate the rest parameters using a linear model
model.0 <- lm(log(index-theta.0) ~ OE, data=exp.test.oe.80)  
alpha.0 <- exp(coef(model.0)[1])
beta.0 <- coef(model.0)[2]

# Starting parameters
start <- list(alpha=alpha.0, beta=beta.0, theta=theta.0)
model.exp.oe <- nls(index ~ alpha*exp(beta*OE) + theta , data=exp.test.oe.80, start=start)
plot(model.exp.oe)
predict(model.exp.oe)

# Predict
exp.test.oe.20$index_modelexp <- NA
exp.test.oe.20$index_modelexp <- predict(model.exp.oe, newdata=exp.test.oe.20)


#---------- PLOTS 
# Plot fitted curve on data points 
expoe<-ggplot(exp.test.oe.80, aes(x=OE, y=index)) +
  geom_point(shape=21, colour="black", fill="white", size=2.5, stroke=1) +
  geom_line(aes(y = predict(model.exp.oe)), size=1, colour="red") +
  annotate(geom="text",x=20, y=17, label="A")

# Population estimate from HPE, from *1.8 and from *model prediction 
expoe.pop<-ggplot() +
  geom_bar(data=exp.test.oe.20, aes(x=factor(n_row), y=hpe), stat="identity", colour="gray60", fill="gray60", width=0.6) +
  geom_bar(data=exp.test.oe.20, aes(x=factor(n_row), y=(lpe*1.8)), stat="identity", colour="black", fill="gray70", alpha=0.5, width=0.5) +
  geom_bar(data=exp.test.oe.20, aes(x=factor(n_row), y=(lpe*index_modelexp)), stat="identity", fill="blue", alpha=0.5, width=0.5) +
  labs(y="Population estimate", x="Survey") +
  annotate(geom="text",x=1, y=260000, label="B")

# Predicted and observed indices
#ggplot() +
#  geom_bar(data=exp.test.oe.20, aes(x=factor(n_row), y=index), stat="identity", colour="black", fill="white") +
#  geom_bar(data=exp.test.oe.20, aes(x=factor(n_row), y=index_modelexp), stat="identity", fill="blue", alpha=0.8, width=0.7) 
```

The relationship is a declining exponential relationship between index and OE, where low OE produces a higher index, which intuitively makes sense (OE is the % of fish you think you can see in a survey) (Fig A5.1a). However, the population estimates derived from the model-predicted expansion factors barely out-performed the traditional *1.8 expansion, and in many cases severly over-estimated the population size (Fig A5.1a).

```{r echo=F, warning=F, message=F}
ggarrange(expoe, expoe.pop, ncol=2)
```

<font size="2"> *Fig A5.1. a) Index~OE with fitted exponential model and b) Comparison of population estimates from the high-precision program (gray bars), the 1.8-expanded population estimate (white with black outline) and model-expanded population estimate (blue).*

<br>

<br>

#### **Index ~ Flow** ####    

The other univariate exercise consisted of fitting a similar exponential model to the relationship of index~discharge (aka flow). The model formula is given below. As above, the model was fit to 80% of the data, while 20% were reserved for model validation (sub-samples chosen randomly). 

```{r eval=F}
Model formula:
model.exp.flow <- nls(index ~ alpha*exp(beta*water_discharge) + theta, data=exp.test.flow.80, start=start)
```

```{r echo=F, include=F}
exp.test.flow <- cal %>% 
  filter(!is.na(index) & !is.na(water_discharge)) %>% 
  print()

set.seed(123)
exp.test.flow.80 <- exp.test.flow %>% 
  slice_sample(prop=0.8) %>%
  print()
exp.flow.names <- exp.test.flow.80$usid

exp.test.flow.20 <- exp.test.flow %>%
  filter(!usid%in%c(exp.flow.names)) %>%
  print()


# Select an approximate $\theta$, since theta must be higher than max(y), and greater than zero
theta.0 <- max(exp.test.flow.80$index)*1.1  

# Estimate the rest parameters using a linear model
model.0 <- lm(log(-index+theta.0) ~ water_discharge, data=exp.test.flow.80)  
alpha.0 <- -exp(coef(model.0)[1])
beta.0 <- coef(model.0)[2]

# Starting parameters
start <- list(alpha=alpha.0, beta=beta.0, theta=theta.0)
model.exp.flow <- nls(index ~ alpha*exp(beta*water_discharge) + theta, data=exp.test.flow.80, start=start)
plot(model.exp.flow)
predict(model.exp.flow)

# Predict
exp.test.flow.20$index_modelexp <- NA
exp.test.flow.20$index_modelexp <- predict(model.exp.flow, newdata=exp.test.flow.20)


#-------- PLOTS
# Plot fitted curve
expfl<-ggplot(exp.test.flow.80, aes(x=water_discharge, y=index)) +
  geom_point(shape=21, colour="black", fill="white", size=2.5, stroke=1) +
  geom_line(aes(y = predict(model.exp.flow)), size=1, colour="red") +
  annotate(geom="text",x=1, y=12, label="A")

# Population estimate from HPE, from *1.8 and from *model prediction  
expfl.pop<-ggplot() +
  geom_bar(data=exp.test.flow.20, aes(x=factor(n_row), y=hpe), stat="identity", colour="gray60", fill="gray60", width=0.6) +
  geom_bar(data=exp.test.flow.20, aes(x=factor(n_row), y=(lpe*1.8)), stat="identity", colour="black", fill="white", alpha=0.5, width=0.5) +
  geom_bar(data=exp.test.flow.20, aes(x=factor(n_row), y=(lpe*index_modelexp)), stat="identity", fill="blue", alpha=0.5, width=0.5) +
  labs(y="Population estimate", x="Survey") +
  annotate(geom="text",x=1, y=450000, label="B")

# Predicted vs. observed indices 
#ggplot() +
#  geom_bar(data=exp.test.flow.20, aes(x=factor(n_row), y=index), stat="identity", colour="black", fill="white") +
#  geom_bar(data=exp.test.flow.20, aes(x=factor(n_row), y=index_modelexp), stat="identity", fill="blue", alpha=0.8, width=0.7) 
```

The relationship between index and OE is an increasing exponential relationship that appears to plateau around index=7 and flow=400-500cms. The relationship is strongly driven by one data point, which could pose issues for future model development (Fig A5.2a). The model performed poorly compared to the traditional 1.8-expansion, and frequently over-estimated the population size (Fig A5.2b).

```{r echo=F, warning=F, message=F}
ggarrange(expfl, expfl.pop, ncol=2)
```

<font size="2"> *Fig A5.2. a) Index~flow with (poorly) fitted exponential model and b) Comparison of population estimates from the high-precision program (gray), the 1.8-expanded population estimate (white with black outline), and the model-expanded population estimate (blue).* </font>

<br>

<br>












